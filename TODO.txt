### Level 1 – Extend your current model (easiest)

With the one-step LSTM you already have, I would:

Implement recursive multi-step:

Start from the last window of the test set,

predict t+1,

feed it back into the window,

predict t+2,

and so on up to t+24 (for example).

Evaluate how the error grows as the horizon increases:

MAE(h=1), MAE(h=2), ..., MAE(h=24).

This teaches you very well:

how models degrade with the horizon,

how to use your current model to simulate further ahead.

### Level 2 – Direct model for 24h

Then, as a next step:

Change the final layer to return a vector of size 24 instead of 1.

Adjust the data so that:

input = 168h of history,

target = the next 24h (vector).

With this you compare:

LSTM one-step + recursive (24 times),

vs LSTM multi-output (24 hours at once).

### Level 3 – If you get the SOTA bug

Only then, later on:

look at architectures like Temporal Fusion Transformer or N-BEATS,

or even pre-trained models like Chronos/T5 adapted for energy.